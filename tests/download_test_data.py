"""
下载测试数据脚本
从公开的中文词汇数据源下载测试数据
"""
import os
import json
import random
import requests
import zipfile
from typing import List, Set
import tempfile
import re


def download_thuocl_data() -> List[str]:
    """下载THUOCL清华大学开放中文词库的部分数据"""
    # 这里使用一些预定义的词汇，模拟从THUOCL下载的数据
    # 实际使用时可以从GitHub API获取真实数据
    
    thuocl_samples = [
        # IT词汇样本
        "文件备份", "虚拟地址", "事务调度", "强连通缩点", "数据库", "算法", "编程",
        "软件工程", "人工智能", "机器学习", "深度学习", "神经网络", "大数据", "云计算",
        "区块链", "物联网", "虚拟现实", "增强现实", "量子计算", "边缘计算",
        
        # 财经词汇样本
        "年期", "调整方案", "全面收购", "差价", "萎缩", "股票", "基金", "证券",
        "投资", "理财", "银行", "保险", "贷款", "利率", "汇率", "通胀", "GDP",
        "经济增长", "货币政策", "财政政策", "金融市场", "资本市场",
        
        # 成语样本
        "故作高深", "有理有据", "用之不竭", "人微言轻", "因地制宜", "求贤若渴",
        "一心一意", "三心二意", "四面八方", "五光十色", "六神无主", "七嘴八舌",
        "八仙过海", "九牛一毛", "十全十美", "百发百中", "千方百计", "万众一心",
        
        # 地名样本
        "北京市", "上海市", "天津市", "重庆市", "河北省", "山西省", "辽宁省",
        "吉林省", "黑龙江省", "江苏省", "浙江省", "安徽省", "福建省", "江西省",
        "山东省", "河南省", "湖北省", "湖南省", "广东省", "海南省", "四川省",
        "贵州省", "云南省", "陕西省", "甘肃省", "青海省", "台湾省",
        "广西壮族自治区", "内蒙古自治区", "西藏自治区", "宁夏回族自治区", "新疆维吾尔自治区",
        "香港特别行政区", "澳门特别行政区",
        
        # 历史名人样本
        "孔子", "老子", "庄子", "孟子", "墨子", "荀子", "韩非子", "商鞅", "管仲",
        "秦始皇", "汉武帝", "唐太宗", "宋太祖", "康熙", "乾隆", "诸葛亮", "关羽",
        "张飞", "赵云", "马超", "黄忠", "周瑜", "鲁迅", "巴金", "老舍", "茅盾",
        
        # 医学词汇样本
        "患者", "充血", "皮疹", "冬虫夏草", "手术", "诊断", "治疗", "药物",
        "病理", "生理", "解剖", "免疫", "遗传", "感染", "炎症", "肿瘤", "癌症",
        "高血压", "糖尿病", "心脏病", "脑卒中", "肺炎", "肝炎", "肾炎",
        
        # 动物样本
        "信鸽", "梅花鹿", "街鸽", "斑尾林鸽", "老虎", "狮子", "大象", "长颈鹿",
        "猴子", "熊猫", "袋鼠", "企鹅", "海豚", "鲸鱼", "鲨鱼", "章鱼", "海龟",
        "老鹰", "麻雀", "燕子", "孔雀", "鸽子", "鸡", "鸭", "鹅", "牛", "羊",
        "猪", "马", "驴", "骆驼", "兔子", "老鼠", "松鼠", "刺猬", "蝙蝠",
    ]
    
    return thuocl_samples


def download_common_chinese_words() -> List[str]:
    """下载常见中文词汇"""
    common_words = [
        # 基础词汇
        "北京", "上海", "广州", "深圳", "杭州", "南京", "武汉", "成都", "重庆", "天津",
        "青岛", "大连", "沈阳", "长春", "哈尔滨", "济南", "郑州", "太原", "呼和浩特",
        "石家庄", "合肥", "南昌", "长沙", "福州", "厦门", "海口", "三亚", "昆明",
        "贵阳", "拉萨", "兰州", "西宁", "银川", "乌鲁木齐", "西安", "宝鸡", "咸阳",
        
        # 水果
        "苹果", "香蕉", "橘子", "橙子", "葡萄", "草莓", "西瓜", "甜瓜", "哈密瓜",
        "桃子", "梨子", "李子", "杏子", "樱桃", "柠檬", "柚子", "荔枝", "龙眼",
        "芒果", "木瓜", "菠萝", "椰子", "榴莲", "猕猴桃", "火龙果", "百香果",
        
        # 蔬菜
        "白菜", "青菜", "菠菜", "韭菜", "芹菜", "生菜", "油菜", "萝卜", "胡萝卜",
        "土豆", "红薯", "山药", "莲藕", "冬瓜", "南瓜", "丝瓜", "苦瓜", "黄瓜",
        "茄子", "番茄", "辣椒", "青椒", "洋葱", "大蒜", "生姜", "大葱", "韭黄",
        
        # 颜色
        "红色", "橙色", "黄色", "绿色", "青色", "蓝色", "紫色", "粉色", "棕色",
        "灰色", "黑色", "白色", "金色", "银色", "彩色", "透明", "深色", "浅色",
        
        # 交通工具
        "汽车", "火车", "飞机", "轮船", "地铁", "公交车", "出租车", "自行车",
        "摩托车", "电动车", "三轮车", "卡车", "货车", "客车", "校车", "救护车",
        "消防车", "警车", "坦克", "直升机", "轰炸机", "战斗机", "航空母舰",
        
        # 建筑
        "房子", "别墅", "公寓", "宿舍", "酒店", "旅馆", "商店", "超市", "商场",
        "百货", "银行", "医院", "学校", "幼儿园", "大学", "图书馆", "博物馆",
        "电影院", "剧院", "体育馆", "游泳馆", "公园", "广场", "车站", "机场",
        
        # 职业
        "老师", "学生", "医生", "护士", "警察", "消防员", "司机", "厨师", "服务员",
        "销售员", "会计", "律师", "工程师", "程序员", "设计师", "记者", "编辑",
        "翻译", "导游", "演员", "歌手", "画家", "作家", "农民", "工人", "商人",
        
        # 动作
        "走路", "跑步", "跳跃", "游泳", "爬山", "骑车", "开车", "坐车", "飞行",
        "吃饭", "喝水", "睡觉", "起床", "洗澡", "刷牙", "洗脸", "梳头", "穿衣",
        "脱衣", "购物", "工作", "学习", "读书", "写字", "画画", "唱歌", "跳舞",
        
        # 时间
        "今天", "昨天", "明天", "前天", "后天", "上午", "下午", "晚上", "中午",
        "早上", "深夜", "凌晨", "白天", "黑夜", "春天", "夏天", "秋天", "冬天",
        "一月", "二月", "三月", "四月", "五月", "六月", "七月", "八月", "九月",
        "十月", "十一月", "十二月", "星期一", "星期二", "星期三", "星期四",
        "星期五", "星期六", "星期日", "周末", "工作日", "节假日",
        
        # 情感
        "开心", "快乐", "高兴", "兴奋", "激动", "满意", "惊喜", "感动", "温暖",
        "幸福", "悲伤", "难过", "伤心", "痛苦", "失望", "绝望", "无奈", "孤独",
        "愤怒", "生气", "恼火", "愤恨", "嫉妒", "羡慕", "担心", "焦虑", "紧张",
        "害怕", "恐惧", "惊吓", "紧张", "放松", "平静", "冷静", "镇定", "自信",
        
        # 形容词
        "大的", "小的", "高的", "矮的", "长的", "短的", "宽的", "窄的", "厚的",
        "薄的", "重的", "轻的", "快的", "慢的", "新的", "旧的", "好的", "坏的",
        "美的", "丑的", "干净", "脏的", "热的", "冷的", "温的", "凉的", "硬的",
        "软的", "光滑", "粗糙", "甜的", "苦的", "酸的", "辣的", "咸的", "淡的",
        
        # 数字
        "一", "二", "三", "四", "五", "六", "七", "八", "九", "十",
        "十一", "十二", "二十", "三十", "四十", "五十", "六十", "七十", "八十", "九十",
        "一百", "二百", "三百", "五百", "一千", "二千", "五千", "一万", "十万", "一百万",
        "零", "第一", "第二", "第三", "第四", "第五", "第六", "第七", "第八", "第九", "第十",
        
        # 方位
        "东", "南", "西", "北", "东南", "东北", "西南", "西北", "中间", "中央",
        "左边", "右边", "前面", "后面", "上面", "下面", "里面", "外面", "旁边",
        "附近", "远处", "近处", "这里", "那里", "哪里", "到处", "处处", "各处",
    ]
    
    return common_words


def generate_extended_vocabulary(base_words: List[str], target_size: int) -> List[str]:
    """基于基础词汇生成扩展词汇"""
    words = set(base_words)
    
    # 常用后缀
    suffixes = ["的", "了", "着", "过", "等", "们", "子", "儿", "头", "性", "化", "者", "员", "师", "家", "手"]
    
    # 常用前缀
    prefixes = ["老", "小", "大", "新", "旧", "好", "坏", "高", "低", "长", "短", "重", "轻", "快", "慢"]
    
    # 组合词
    connectors = ["和", "与", "及", "或", "但", "而", "却", "还", "也", "都", "又", "再", "就", "才", "只", "已"]
    
    while len(words) < target_size:
        base_word = random.choice(base_words)
        
        # 随机选择一种扩展方式
        method = random.choice(['suffix', 'prefix', 'combine', 'repeat'])
        
        if method == 'suffix' and suffixes:
            new_word = base_word + random.choice(suffixes)
        elif method == 'prefix' and prefixes:
            new_word = random.choice(prefixes) + base_word
        elif method == 'combine':
            other_word = random.choice(base_words)
            connector = random.choice(connectors) if random.random() < 0.3 else ""
            new_word = base_word + connector + other_word
        else:
            # 重复字符或修改
            if len(base_word) >= 2:
                # 去掉最后一个字符
                new_word = base_word[:-1]
            else:
                # 重复字符
                new_word = base_word + base_word[-1]
        
        # 确保词汇长度合理
        if 1 <= len(new_word) <= 10:
            words.add(new_word)
    
    return list(words)


def create_test_dataset(size: int) -> List[str]:
    """创建指定大小的测试数据集"""
    print(f"创建 {size:,} 个词汇的测试数据集...")
    
    # 收集基础词汇
    base_words = []
    base_words.extend(download_thuocl_data())
    base_words.extend(download_common_chinese_words())
    
    # 去重
    base_words = list(set(base_words))
    print(f"收集到 {len(base_words)} 个基础词汇")
    
    if len(base_words) >= size:
        return base_words[:size]
    
    # 扩展词汇
    print("扩展词汇...")
    import random
    extended_words = generate_extended_vocabulary(base_words, size)
    
    return extended_words[:size]


def save_test_datasets():
    """保存不同规模的测试数据集"""
    datasets_dir = "test_datasets"
    os.makedirs(datasets_dir, exist_ok=True)
    
    sizes = [1000, 5000, 10000, 50000, 100000]
    
    for size in sizes:
        filename = os.path.join(datasets_dir, f"chinese_words_{size}.json")
        if not os.path.exists(filename):
            print(f"\n创建 {size:,} 词汇数据集...")
            words = create_test_dataset(size)
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(words, f, ensure_ascii=False, indent=2)
            
            print(f"保存到: {filename}")
        else:
            print(f"数据集已存在: {filename}")
    
    print(f"\n所有测试数据集已保存到 {datasets_dir} 目录")


if __name__ == "__main__":
    save_test_datasets() 